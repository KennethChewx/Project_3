{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "\n",
    "# Display the max for string\n",
    "pd.set_option('max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset='title',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tea       986\n",
       "Coffee    927\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.clker.com/cliparts/k/L/Z/0/Q/x/red-check-mark.svg\" style=\"float: left; margin: 0px 0px 0px 0px; height: 20px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As many posts have images in their self text, we will include titles in selftext to make up the text column\n",
    "# Add title to selftext to fill up empty rows\n",
    "df[\"title_text\"] = df[\"title\"].map(str) + ' ' + df[\"selftext\"].map(str)\n",
    "# Replacing newlines and /r/ reddit addition to subreddits\n",
    "df = df.replace('\\n','', regex=True)\n",
    "df = df.replace('/r/','', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title_text'].values.astype('str')\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification. Predicting Tea as our target as opposed to coffee\n",
    "y.replace('Coffee', 0,inplace=True)\n",
    "y.replace('tea',1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "token = RegexpTokenizer(r'\\w+')\n",
    "X_token = df['title_text'].apply(token.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words\n",
    "X_token_stop = X_token.apply(lambda x : [i.lower() for i in x if not i.lower() in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize\n",
    "lem = WordNetLemmatizer()\n",
    "X_token_stop_lem = X_token_stop.apply(lambda x : [lem.lemmatize(str(i)) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_token_stop_lem, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.clker.com/cliparts/k/L/Z/0/Q/x/red-check-mark.svg\" style=\"float: left; margin: 0px 0px 0px 0px; height: 20px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(lowercase=False,\n",
    "                             analyzer = \"word\")),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'cvec', 'tfidf', 'lr', 'cvec__analyzer', 'cvec__binary', 'cvec__decode_error', 'cvec__dtype', 'cvec__encoding', 'cvec__input', 'cvec__lowercase', 'cvec__max_df', 'cvec__max_features', 'cvec__min_df', 'cvec__ngram_range', 'cvec__preprocessor', 'cvec__stop_words', 'cvec__strip_accents', 'cvec__token_pattern', 'cvec__tokenizer', 'cvec__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'lr__C', 'lr__class_weight', 'lr__dual', 'lr__fit_intercept', 'lr__intercept_scaling', 'lr__l1_ratio', 'lr__max_iter', 'lr__multi_class', 'lr__n_jobs', 'lr__penalty', 'lr__random_state', 'lr__solver', 'lr__tol', 'lr__verbose', 'lr__warm_start'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'cvec__max_features': [500,1000,1500,2000,2500, 3000, 3500],\n",
    "    'cvec__min_df': [2, 3, 4],\n",
    "    'cvec__max_df': [.75, .85, .9, .95],\n",
    "    'cvec__ngram_range': [(1, 1),(1,2)],\n",
    "    'lr__l1_ratio': np.linspace(0,1,6),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1008 candidates, totalling 5040 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   35.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:   54.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 5040 out of 5040 | elapsed:  3.7min finished\n",
      "C:\\Users\\n3rDx\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\n3rDx\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1506: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  \"(penalty={})\".format(self.penalty))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cvec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=False,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents=None,\n",
       "                                                        to...\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'cvec__max_df': [0.75, 0.85, 0.9, 0.95],\n",
       "                         'cvec__max_features': [500, 1000, 1500, 2000, 2500,\n",
       "                                                3000, 3500],\n",
       "                         'cvec__min_df': [2, 3, 4],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'lr__l1_ratio': array([0. , 0.2, 0.4, 0.6, 0.8, 1. ])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=5, n_jobs=-1,verbose = 1)\n",
    "gs.fit(X_train.astype(str), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9804741980474198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.75,\n",
       " 'cvec__max_features': 1000,\n",
       " 'cvec__min_df': 2,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'lr__l1_ratio': 0.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9909344490934449"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train.astype(str), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9812108559498957"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test.astype(str), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs.predict(X_test.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       232\n",
      "           1       0.99      0.97      0.98       247\n",
      "\n",
      "    accuracy                           0.98       479\n",
      "   macro avg       0.98      0.98      0.98       479\n",
      "weighted avg       0.98      0.98      0.98       479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.clker.com/cliparts/k/L/Z/0/Q/x/red-check-mark.svg\" style=\"float: left; margin: 0px 0px 0px 0px; height: 20px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other model\n",
    "pipe2 = Pipeline([\n",
    "    ('cvec', CountVectorizer(lowercase=False, strip_accents='unicode', analyzer = \"word\")),\n",
    "    ('multinb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('cvec',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                   dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                   lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "                   ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                   strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                   tokenizer=None, vocabulary=None)),\n",
       "  ('multinb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       " 'verbose': False,\n",
       " 'cvec': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                 lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "                 ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                 strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, vocabulary=None),\n",
       " 'multinb': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " 'cvec__analyzer': 'word',\n",
       " 'cvec__binary': False,\n",
       " 'cvec__decode_error': 'strict',\n",
       " 'cvec__dtype': numpy.int64,\n",
       " 'cvec__encoding': 'utf-8',\n",
       " 'cvec__input': 'content',\n",
       " 'cvec__lowercase': False,\n",
       " 'cvec__max_df': 1.0,\n",
       " 'cvec__max_features': None,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__preprocessor': None,\n",
       " 'cvec__stop_words': None,\n",
       " 'cvec__strip_accents': 'unicode',\n",
       " 'cvec__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'cvec__tokenizer': None,\n",
       " 'cvec__vocabulary': None,\n",
       " 'multinb__alpha': 1.0,\n",
       " 'multinb__class_prior': None,\n",
       " 'multinb__fit_prior': True}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2_params = {\n",
    "    'cvec__max_features': [500,1000,1500,2000,2500, 3000, 3500],\n",
    "    'cvec__min_df': [2, 3, 4],\n",
    "    'cvec__max_df': [.75, .85, .9, .95],\n",
    "    'cvec__ngram_range': [(1, 1),(1,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 168 candidates, totalling 840 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   20.6s\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:   35.4s\n",
      "[Parallel(n_jobs=-1)]: Done 840 out of 840 | elapsed:   37.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('cvec',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=False,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accents='unicod...\n",
       "                                       ('multinb',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'cvec__max_df': [0.75, 0.85, 0.9, 0.95],\n",
       "                         'cvec__max_features': [500, 1000, 1500, 2000, 2500,\n",
       "                                                3000, 3500],\n",
       "                         'cvec__min_df': [2, 3, 4],\n",
       "                         'cvec__ngram_range': [(1, 1), (1, 2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2 = GridSearchCV(pipe2, param_grid=pipe2_params, cv=5, n_jobs=-1,verbose = 1)\n",
    "gs2.fit(X_train.astype(str), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9811715481171548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.75,\n",
       " 'cvec__max_features': 2000,\n",
       " 'cvec__min_df': 4,\n",
       " 'cvec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gs2.best_score_)\n",
    "gs2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9853556485355649"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.score(X_train.astype(str), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9749478079331941"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.score(X_test.astype(str), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs2.predict(X_test.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       232\n",
      "           1       0.98      0.97      0.98       247\n",
      "\n",
      "    accuracy                           0.97       479\n",
      "   macro avg       0.97      0.98      0.97       479\n",
      "weighted avg       0.98      0.97      0.97       479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.clker.com/cliparts/k/L/Z/0/Q/x/red-check-mark.svg\" style=\"float: left; margin: 0px 0px 0px 0px; height: 20px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = red> Comments<br>\n",
    "1. Analyse the results and choose the model.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
